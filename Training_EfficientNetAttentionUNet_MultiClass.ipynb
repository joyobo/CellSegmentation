{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gNLQvPTKpOHV",
    "outputId": "02a1d487-c072-47a1-edfe-c2d14c65dfe4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ri0wzjpOF03D",
    "outputId": "371ec96f-98f2-4b71-e7be-352127aef29c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/CellCount\n"
     ]
    }
   ],
   "source": [
    "# %cd /content/drive/MyDrive/Term 8/Term 8 Artificial Intelligence/folder\n",
    "%cd /content/drive/MyDrive/CellCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0KhkJi2uFlMD",
    "outputId": "0dcf58da-eaf7-4f1b-a8fe-d5ec59cb6b13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path './earlystopping' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Bjarten/early-stopping-pytorch.git ./earlystopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lyH-MERAYQR2",
    "outputId": "1106fd86-a688-4f87-dfc7-10f837bd57c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: efficientnet_pytorch in /usr/local/lib/python3.7/dist-packages (0.7.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet_pytorch) (1.9.0+cu102)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet_pytorch) (3.7.4.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install efficientnet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "US40j-7RoYYw"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import datetime\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from model import *\n",
    "from utils import *\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from skimage import io\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from scipy.io import loadmat, savemat\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from earlystopping.pytorchtools import EarlyStopping\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwZAWxnvq04l"
   },
   "source": [
    "# Start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "R2HRFeuVoYZe"
   },
   "outputs": [],
   "source": [
    "def to_categorical(y, num_classes):\n",
    "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
    "    return np.eye(num_classes, dtype='uint8')[y]\n",
    "\n",
    "class CellDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.fileNames = [i[:-4] for i in sorted(os.listdir(root_dir+\"/Labels/\"))]\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fileNames)\n",
    "    \n",
    "    \"\"\"\n",
    "    returns: tuple of\n",
    "    - image\n",
    "    - mask of size n x n with unique values/classes ranging from 0 to 4 \n",
    "        0: background\n",
    "        1: others (1)\n",
    "        2: inflammatory(2)\n",
    "        3: healthy epithelial(3) , dysplastic/malignant epithelial(4)\n",
    "        4: fibroblast(5) , muscle(6) , endothelial(7)\n",
    "    \"\"\"\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir,\"Images\",self.fileNames[idx])+\".png\"\n",
    "        image = img = Image.open(img_name).convert('RGB')\n",
    "        mask_name = os.path.join(self.root_dir,\"Labels\",self.fileNames[idx])+\".mat\"\n",
    "        x = loadmat(mask_name)['type_map']\n",
    "        x[(x==3)|(x==4)]=3\n",
    "        x[(x==5)|(x==6)|(x==7)]=4\n",
    "        x=np.pad(x.astype(int),6)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, to_categorical(x,5).transpose(2, 0, 1) #(num_classes=5, n, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "oql6fi9poYZf"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Pad(6),\n",
    "    transforms.ToTensor()    \n",
    "])\n",
    "\n",
    "valid_size = 0.2\n",
    "batch_size =1\n",
    "\n",
    "# DataLoader\n",
    "train_data = CellDataset(root_dir=\"../Train/train_500\", transform = transform)\n",
    "test_data = CellDataset(root_dir=\"../Test/test_500\", transform = transform)\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# load training data in batches\n",
    "train_loader = DataLoader(train_data,\n",
    "                          batch_size=batch_size,\n",
    "                          sampler=train_sampler,\n",
    "                          num_workers=0)\n",
    "\n",
    "# load validation data in batches\n",
    "valid_loader = DataLoader(train_data,\n",
    "                          batch_size=batch_size,\n",
    "                          sampler=valid_sampler,\n",
    "                          num_workers=0)\n",
    "\n",
    "# load test data in batches\n",
    "test_loader = DataLoader(test_data,\n",
    "                        batch_size=batch_size,\n",
    "                        num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UsikYua8oYZg"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "m1cCDQzFoYZh"
   },
   "outputs": [],
   "source": [
    "def weighted_loss(pred,targ,bce_weight=0.5, smooth=1.):\n",
    "    \n",
    "    bce = F.binary_cross_entropy_with_logits(pred.squeeze(dim=1), targ)\n",
    "    \n",
    "    pred = torch.sigmoid(pred)\n",
    "    \n",
    "    pred = pred.contiguous().squeeze(dim=1)  \n",
    "    targ = targ.contiguous()  \n",
    "\n",
    "    intersection = (pred * targ).sum(dim=1).sum(dim=1)\n",
    "    dice = (1 - ((2. * intersection + smooth) / (pred.sum(dim=1).sum(dim=1) + targ.sum(dim=1).sum(dim=1) + smooth)))\n",
    "    \n",
    "    loss = bce * bce_weight + dice.mean() * (1 - bce_weight)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def train(model, device, train_loader, valid_loader, optimizer, epochs, patience, img_size):\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    avg_train_losses = [] # average training loss per epoch\n",
    "    avg_valid_losses = [] # average validation loss per epoch\n",
    "    valid_acc = 0\n",
    "    \n",
    "    os.makedirs(\"best_model_checkpoints\", exist_ok=True)\n",
    "    os.makedirs(\"model_checkpoints\", exist_ok=True)\n",
    "    save_path = f\"best_model_checkpoints/{model.__class__.__name__}.pth\"\n",
    "    early_stopping = EarlyStopping(patience=patience, path=save_path, verbose=True)\n",
    "    for epoch in tqdm(range(1, epochs + 1)):\n",
    "        train_loss = 0\n",
    "        valid_loss = 0\n",
    "        score = 0\n",
    "        \"\"\"\n",
    "        Trains the model on training data\n",
    "        \"\"\"\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            data, target = data.to(device), target.to(device).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = weighted_loss(output,target,bce_weight=0.5)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss+=(loss/len(data)).item()\n",
    "            train_losses.append(loss.item())\n",
    "            # Find accuracy\n",
    "        \n",
    "        \"\"\"\n",
    "        Validate the model on validation data\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        for batch_idx, (data, target) in enumerate(valid_loader):\n",
    "            data, target = data.to(device), target.to(device).float()\n",
    "            output = model(data)\n",
    "            loss = weighted_loss(output,target,bce_weight=0.3)\n",
    "            valid_loss+=(loss/len(data)).item()\n",
    "            valid_losses.append(loss.item())\n",
    "            pred = output.to('cpu').detach().numpy()[0][0]\n",
    "            pred[pred.max(axis=0,keepdims=1) == pred] = 1\n",
    "            pred[pred.max(axis=0,keepdims=1) != pred] = 0\n",
    "            score+=get_dice_1(target, pred)\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        valid_loss /= len(valid_loader)\n",
    "        score /= len(valid_loader)\n",
    "        if score > valid_acc:\n",
    "            valid_acc = score\n",
    "\n",
    "        # average loss over an epoch\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "\n",
    "        # clear lists to track next epoch\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "\n",
    "        # early_stopping needs the validation loss to check if it has decresed, \n",
    "        # and if it has, it will make a checkpoint of the current model\n",
    "        early_stopping(valid_loss, model)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "        torch.save(model.state_dict(), f\"model_checkpoints/{model.__class__.__name__}_{img_size}_{epoch}.pth\")\n",
    "        print('Train Epoch: {} @ {} - Train Loss: {:.4f} - Valid Loss: {:.4f}'.format(epoch, datetime.datetime.now().time(), train_loss, valid_loss))\n",
    "\n",
    "    # load the last checkpoint with the best model\n",
    "    model.load_state_dict(torch.load(save_path)) \n",
    "\n",
    "    return  model, avg_train_losses, avg_valid_losses, valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g85aEsploYZj",
    "outputId": "7c899232-73fd-49fe-84a5-a449035965b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "# Set cpu / gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = build_model(device, \"efficientnet\", EfficientNet)\n",
    "model.to(device)\n",
    "\n",
    "lr = 0.001\n",
    "optimizer = Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6-XwZQyFHA_L",
    "outputId": "52bfff23-f33f-43d0-d056-2f1d8aab74e9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Validation loss decreased (inf --> 0.177265).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  1%|          | 1/100 [00:47<1:17:39, 47.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 @ 09:23:04.084632 - Train Loss: 0.2559 - Valid Loss: 0.1773\n",
      "Validation loss decreased (0.177265 --> 0.137654).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  2%|▏         | 2/100 [01:23<1:11:28, 43.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 @ 09:23:40.131999 - Train Loss: 0.1587 - Valid Loss: 0.1377\n",
      "Validation loss decreased (0.137654 --> 0.128781).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  3%|▎         | 3/100 [01:58<1:06:48, 41.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 @ 09:24:15.767743 - Train Loss: 0.1445 - Valid Loss: 0.1288\n"
     ]
    }
   ],
   "source": [
    "print(\"Training\")\n",
    "# Training the model\n",
    "epochs = 100\n",
    "patience = 10  # how long to wait after last time validation loss improved\n",
    "img_size = \"EfficientNet-500x500\"\n",
    "\n",
    "model, train_loss, valid_loss, valid_acc = train(model, device, train_loader, valid_loader, optimizer, epochs, patience, img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qUYlWwIhXe5M"
   },
   "outputs": [],
   "source": [
    "print(\"DICE score:\", valid_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "496-1NjDkOhd"
   },
   "source": [
    "# Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Go28Ud9878Oz"
   },
   "outputs": [],
   "source": [
    "def plot_graph(train_loss, valid_loss):\n",
    "  fig = plt.figure(figsize=(10,8))\n",
    "  plt.plot(range(1,len(train_loss)+1),train_loss, label='Training Loss')\n",
    "  plt.plot(range(1,len(valid_loss)+1),valid_loss,label='Validation Loss')\n",
    "\n",
    "  # find position of lowest validation loss\n",
    "  minposs = valid_loss.index(min(valid_loss))+1 \n",
    "  plt.axvline(minposs, linestyle='--', color='r',label='Early Stopping Checkpoint')\n",
    "\n",
    "  plt.xlabel('epochs')\n",
    "  plt.ylabel('loss')\n",
    "  plt.ylim(0, 0.5) # consistent scale\n",
    "  plt.xlim(0, len(train_loss)+1) # consistent scale\n",
    "  plt.grid(True)\n",
    "  plt.legend()\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "  fig.savefig('loss_plot.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SKPVMQHlkQSh"
   },
   "outputs": [],
   "source": [
    "plot_graph(train_loss, valid_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cy8OdumOVIar"
   },
   "source": [
    "# Load model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "26rd63IGVMrF"
   },
   "outputs": [],
   "source": [
    "# load saved model\n",
    "def load_model(model):\n",
    "  path = f\"best_model_checkpoints/{model.__class__.__name__}.pth\"\n",
    "  model.load_state_dict(torch.load(path)) \n",
    "  model.eval()\n",
    "  return model\n",
    "model = load_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-QK3yY7lGbc"
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eMKJo213Xe5O"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "0: black: background\n",
    "1: red: others (1)\n",
    "2: green: inflammatory(2)\n",
    "3: dark blue: healthy epithelial(3) , dysplastic/malignant epithelial(4)\n",
    "4: light blue: fibroblast(5) , muscle(6) , endothelial(7)\n",
    "\"\"\"\n",
    "# params: 5 x n x n numpy or n x n x 5\n",
    "def printColoredMask(npMask,numchannel=5):\n",
    "    if npMask.shape[-1]!=5:\n",
    "        npMask=npMask.transpose(1, 2, 0)\n",
    "    finalnpMask=np.where(npMask[:,:,1]==1,255,0) # one color\n",
    "    finalnpMask=finalnpMask[:,:,None]\n",
    "    temp=np.where((npMask[:,:,2]==1)|(npMask[:,:,4]==1),255,0) # one color\n",
    "    finalnpMask = np.concatenate((finalnpMask,temp[:, :, None]),axis=2)\n",
    "    temp=np.where((npMask[:,:,3]==1)|(npMask[:,:,4]==1),255,0) # one color\n",
    "    finalnpMask = np.concatenate((finalnpMask,temp[:, :, None]),axis=2)\n",
    "    plt.imshow(finalnpMask)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cy3v6P2k0gqn"
   },
   "outputs": [],
   "source": [
    "def test(model, device, data, target):\n",
    "    print(\"Input Image\")\n",
    "    plt.imshow(data[0].numpy().transpose(1, 2, 0))\n",
    "    plt.show()\n",
    "    outputs = model(data.to(device))[0]\n",
    "    pred = outputs.to('cpu').detach()\n",
    "    pred=F.softmax(pred, dim=0)# along the channel\n",
    "    pred=pred.numpy()\n",
    "\n",
    "    print(\"Predicted Mask Sigmoid\")\n",
    "    pred[pred.max(axis=0,keepdims=1) == pred] = 1\n",
    "    pred[pred.max(axis=0,keepdims=1) != pred] = 0\n",
    "    printColoredMask(pred)\n",
    "    print(\"Actual Mask\")\n",
    "    printColoredMask(target[0].numpy())\n",
    "\n",
    "    # PQ\n",
    "    pq_score = get_fast_pq(target, pred)[0]\n",
    "    print(\"Detection Quality (DQ):\", pq_score[0])\n",
    "    print(\"Segmentation Quality (SQ):\", pq_score[1])\n",
    "    print(\"Panoptic Quality (PQ):\", pq_score[2])\n",
    "    \n",
    "    dice_score = get_dice_1(target, pred)\n",
    "    print(\"Dice score:\", dice_score, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EhR_iX6gPIed"
   },
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "for data, target in test_loader:\n",
    "    test(model, device, data, target)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Training-EfficientNetAttentionUNet-MultiClass.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
